{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b7952eb-ff6f-42de-9ec8-eb59cddf0452",
   "metadata": {},
   "source": [
    "## Question - 1\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbf6a28-dba9-4d46-9fd9-20c19ba462db",
   "metadata": {},
   "source": [
    "Gradient Boosting Regression is a machine learning technique used for building regression models. It is a type of ensemble learning method that combines the predictions of multiple weak learners (typically decision trees) sequentially to create a strong predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d78d98-1aa9-4aff-8b74-b35ad9b2789b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e09849-5bae-4503-83d4-4391bd31da9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e2c3689-386c-4b31-896b-d56a8931129e",
   "metadata": {},
   "source": [
    "## Question - 2\n",
    "ans - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6cb87f6-7540-4999-a481-d74b58e59586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingRegressor()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "\n",
    "x, y = make_regression(n_samples = 1000 , n_features = 2 , noise=0.5 , random_state=0)\n",
    "\n",
    "\n",
    "x_train , x_test , y_train , y_test = train_test_split(x, y , test_size= 0.20 , random_state = 0)\n",
    "\n",
    "\n",
    "\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100 , learning_rate=0.1)\n",
    "\n",
    "gb_regressor.fit(x_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5a22b2d-6d6b-4015-975f-9b519011b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gb_regressor.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee12b4da-7171-4a28-9128-c9985863934b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9930457722283429\n",
      "19.958906902781127\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score , mean_squared_error\n",
    "\n",
    "\n",
    "print(r2_score(y_pred , y_test))\n",
    "print(mean_squared_error(y_pred , y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c870f5-bb30-4a40-b904-17b49d53da05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fdeae0-3054-46d0-9356-1a80420d35da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0416de80-1edc-4aa9-8cb2-ec0c444ae5c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6ab5ffe-5aad-4cba-b910-e6ae1b68afc2",
   "metadata": {},
   "source": [
    "## Question - 3\n",
    "ans - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34e16371-56ff-4526-99cc-cab4fa2ca718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "parameters = {'loss':['squared_error', 'absolute_error'],\n",
    "             'learning_rate':[0.1],\n",
    "             'n_estimators':[50,100,150],\n",
    "             'criterion':['friedman_mse' , 'squared_error'],\n",
    "             'max_depth':[3,4,5],\n",
    "             'alpha':[0.3,0.6]}\n",
    "\n",
    "\n",
    "grid_regressor = GridSearchCV(GradientBoostingRegressor() , param_grid= parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b252aaa-1103-402a-9b9f-8e0f76558522",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=GradientBoostingRegressor(),\n",
       "             param_grid={&#x27;alpha&#x27;: [0.3, 0.6],\n",
       "                         &#x27;criterion&#x27;: [&#x27;friedman_mse&#x27;, &#x27;squared_error&#x27;],\n",
       "                         &#x27;learning_rate&#x27;: [0.1],\n",
       "                         &#x27;loss&#x27;: [&#x27;squared_error&#x27;, &#x27;absolute_error&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [3, 4, 5],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 150]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=GradientBoostingRegressor(),\n",
       "             param_grid={&#x27;alpha&#x27;: [0.3, 0.6],\n",
       "                         &#x27;criterion&#x27;: [&#x27;friedman_mse&#x27;, &#x27;squared_error&#x27;],\n",
       "                         &#x27;learning_rate&#x27;: [0.1],\n",
       "                         &#x27;loss&#x27;: [&#x27;squared_error&#x27;, &#x27;absolute_error&#x27;],\n",
       "                         &#x27;max_depth&#x27;: [3, 4, 5],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 150]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=GradientBoostingRegressor(),\n",
       "             param_grid={'alpha': [0.3, 0.6],\n",
       "                         'criterion': ['friedman_mse', 'squared_error'],\n",
       "                         'learning_rate': [0.1],\n",
       "                         'loss': ['squared_error', 'absolute_error'],\n",
       "                         'max_depth': [3, 4, 5],\n",
       "                         'n_estimators': [50, 100, 150]})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_regressor.fit(x_train , y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d373f66-7803-4382-961d-594e47344cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.6,\n",
       " 'criterion': 'squared_error',\n",
       " 'learning_rate': 0.1,\n",
       " 'loss': 'squared_error',\n",
       " 'max_depth': 3,\n",
       " 'n_estimators': 150}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_regressor.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7577769f-3aca-48cd-aefb-bee1a61cad26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a91f0fc0-1bee-4887-92c3-5f8971c1495f",
   "metadata": {},
   "source": [
    "## Question - 4\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93496d1e-f2a0-4c24-bcdf-05dbbfd5596f",
   "metadata": {},
   "source": [
    "In the context of gradient boosting, a weak learner refers to a simple predictive model that performs slightly better than random guessing on a given learning task. Weak learners are typically simple models with limited complexity, such as shallow decision trees or linear models.\n",
    "\n",
    "* Characteristics of Weak Learners:\n",
    "\n",
    "1. Limited Complexity:\n",
    "\n",
    "Weak learners are intentionally kept simple to prevent overfitting and improve generalization performance.\n",
    "For example, decision trees with a small number of nodes or depth are commonly used as weak learners in gradient boosting.\n",
    "\n",
    "\n",
    "2. Performance Slightly Better Than Random:\n",
    "\n",
    "While weak learners may not perform well on their own, they should still provide some predictive power that is slightly better than random guessing.\n",
    "Weak learners are often referred to as \"weak\" because their individual predictive performance is modest compared to more complex models.\n",
    "\n",
    "\n",
    "3. Ensemble Learning:\n",
    "\n",
    "In gradient boosting, weak learners are combined into a strong predictive model by sequentially adding them to the ensemble and optimizing their contributions to minimize the overall error of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7eb70e-29c9-4423-99d1-50a20b3ced1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b453e6-4117-4706-9833-e2a868996799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af170703-f956-4aab-bfb1-1433e9e367d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d9b964b-f51a-4cda-98a5-44f67ecd5a57",
   "metadata": {},
   "source": [
    "## Question - 5\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc50be4-c42d-4893-98b5-946633ee4137",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm can be understood by breaking down its key components and how they work together to build a strong predictive model. Here's a simplified explanation of the intuition behind Gradient Boosting:\n",
    "\n",
    "1. Weak Learners:\n",
    "Gradient Boosting combines multiple weak learners (typically decision trees) sequentially to form a strong ensemble model.\n",
    "Each weak learner is trained on the residuals (errors) of the previous learners, focusing on capturing the remaining patterns in the data that were not captured by the previous models.\n",
    "\n",
    "2. Sequential Model Building:\n",
    "The algorithm builds the ensemble model iteratively, adding one weak learner at a time.\n",
    "At each iteration, a new weak learner is trained to correct the errors made by the current ensemble of models.\n",
    "\n",
    "3. Gradient Descent:\n",
    "Gradient Boosting uses a gradient descent optimization technique to minimize the loss function (e.g., mean squared error) of the model.\n",
    "At each iteration, the algorithm calculates the gradient of the loss function with respect to the predictions made by the current ensemble model.\n",
    "It then fits a new weak learner to the negative gradient (residuals) of the loss function, effectively moving the model in the direction that minimizes the loss.\n",
    "\n",
    "4. Additive Model:\n",
    "The predictions of the ensemble model are the sum of the predictions made by all the individual weak learners.\n",
    "Each weak learner contributes a small amount to the overall prediction, and the contributions are weighted by a learning rate parameter.\n",
    "\n",
    "5. Regularization:\n",
    "Gradient Boosting incorporates regularization techniques to prevent overfitting and improve model generalization.\n",
    "Regularization is achieved through techniques such as shrinkage (learning rate), limiting the complexity of weak learners, and adding constraints on the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1a1ed0-5534-46b0-a115-f416bf695f6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3bcce2-f4d5-4504-8c7c-e01c8bfa74cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e5513f-953c-4387-a225-e09a6b896dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdff8432-cb93-4d16-a3c2-e0aed5769f43",
   "metadata": {},
   "source": [
    "## Question - 6\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83a247c-f406-42f4-af4d-1f7341ea2070",
   "metadata": {},
   "source": [
    "## 1. Initialization:\n",
    "\n",
    "The algorithm starts by initializing the ensemble model with a simple weak learner, typically a decision tree with shallow depth (few nodes) or even a single node.\n",
    "This initial weak learner makes predictions based on the average of the target variable (for regression) or the majority class (for classification).\n",
    "\n",
    "## 2. Iterative Training:\n",
    "\n",
    "At each iteration (or boosting round), the algorithm adds a new weak learner to the ensemble.\n",
    "The new weak learner is trained to predict the residuals (errors) of the current ensemble model rather than the original target variable.\n",
    "The residuals are the differences between the actual target values and the predictions made by the current ensemble model.\n",
    "\n",
    "## 3. Residual Calculation:\n",
    "\n",
    "After each iteration, the residuals are updated based on the difference between the actual target values and the predictions made by the current ensemble model.\n",
    "The residuals represent the part of the target variable that has not been captured by the current ensemble model.\n",
    "\n",
    "## 4. Gradient Descent Optimization:\n",
    "\n",
    "Gradient Boosting uses a gradient descent optimization technique to minimize the loss function (e.g., mean squared error for regression or cross-entropy loss for classification).\n",
    "At each iteration, the algorithm calculates the negative gradient of the loss function with respect to the predictions made by the current ensemble model.\n",
    "It then fits a new weak learner to the negative gradient (residuals), effectively moving the model in the direction that minimizes the loss.\n",
    "\n",
    "## 5. Weighted Sum of Predictions:\n",
    "\n",
    "The predictions of the ensemble model are the weighted sum of the predictions made by all the individual weak learners.\n",
    "Each weak learner contributes a small amount to the overall prediction, and the contributions are weighted by a learning rate parameter.\n",
    "\n",
    "## 6. Regularization:\n",
    "\n",
    "Gradient Boosting incorporates regularization techniques to prevent overfitting and improve model generalization.\n",
    "Regularization is achieved through techniques such as shrinkage (learning rate), limiting the complexity of weak learners, and adding constraints on the model parameters.\n",
    "\n",
    "## 7. Stopping Criterion:\n",
    "\n",
    "The algorithm continues to add weak learners to the ensemble until a stopping criterion is met, such as reaching a maximum number of iterations, achieving a minimum improvement in the loss function, or reaching a maximum depth for the weak learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb56eba3-5c70-46f4-bad0-17479cae72dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33809e1d-ffe5-434b-904b-a380af54b842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6b8aac7-905c-4ead-985c-339e82f92b9e",
   "metadata": {},
   "source": [
    "## Question - 7\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffef4676-3068-4ef3-ab8c-c3ff4d046abb",
   "metadata": {},
   "source": [
    "Constructing the mathematical intuition behind the Gradient Boosting algorithm involves understanding the underlying principles of gradient descent optimization, the concept of residuals, and the additive nature of the ensemble model. Here are the key steps involved in building the mathematical intuition of Gradient Boosting:\n",
    "\n",
    "## 1. Loss Function:\n",
    "\n",
    "* Start with a defined loss function that measures the discrepancy between the actual target values and the predictions made by the model.\n",
    "\n",
    "* For regression problems, the loss function could be mean squared error (MSE), while for classification problems, it could be cross-entropy loss.\n",
    "\n",
    "## 2. Gradient Descent Optimization:\n",
    "\n",
    "* Understand the concept of gradient descent, which is an optimization algorithm used to minimize the loss function.\n",
    "\n",
    "* Gradient descent iteratively updates the model parameters (weights) in the direction that reduces the loss function the most.\n",
    "\n",
    "* Calculate the gradient of the loss function with respect to the model predictions to determine the direction of steepest descent.\n",
    "\n",
    "\n",
    "## 3. Residuals:\n",
    "\n",
    "* Define the residuals as the differences between the actual target values and the predictions made by the current model.\n",
    "\n",
    "* Residuals represent the errors or discrepancies in the predictions that the model has not yet captured.\n",
    "\n",
    "\n",
    "## 4. Weak Learners:\n",
    "\n",
    "* Introduce the concept of weak learners, which are simple models (e.g., decision trees) that perform slightly better than random guessing.\n",
    "\n",
    "* Each weak learner is trained to predict the residuals of the current model rather than the original target values.\n",
    "\n",
    "* Weak learners focus on capturing the remaining patterns in the data that were not captured by the previous models.\n",
    "\n",
    "\n",
    "## 5. Additive Model:\n",
    "\n",
    "* Understand that Gradient Boosting constructs an ensemble model by sequentially adding weak learners to the ensemble.\n",
    "\n",
    "* Each weak learner contributes a small amount to the overall prediction, and the contributions are weighted by a learning rate parameter.\n",
    "\n",
    "* The predictions of the ensemble model are the sum of the predictions made by all the individual weak learners.\n",
    "\n",
    "## 6. Gradient Boosting Algorithm:\n",
    "\n",
    "* Combine the concepts of gradient descent optimization, residuals, weak learners, and additive model to formulate the Gradient Boosting algorithm.\n",
    "\n",
    "* At each iteration, calculate the negative gradient of the loss function with respect to the predictions made by the current ensemble model.\n",
    "\n",
    "* Fit a new weak learner to the negative gradient (residuals), effectively moving the model in the direction that minimizes the loss.\n",
    "\n",
    "## 7. Regularization:\n",
    "\n",
    "* Consider the incorporation of regularization techniques in Gradient Boosting to prevent overfitting and improve model generalization.\n",
    "\n",
    "* Regularization techniques may include shrinkage (learning rate), limiting the complexity of weak learners, and adding constraints on the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafc36ea-dd4b-4e63-9fd9-e6c346690d1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
